

\documentclass[11pt,a4paper]{article} 
\setcounter{secnumdepth}{3}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{pdflscape}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[margin=1.5in]{geometry}
\usepackage{caption}
\usepackage[font=footnotesize]{caption}
\usepackage{apacite}
\usepackage{color}
\usepackage[justification=centering]{caption}
\usepackage{amsmath}
\usepackage{booktabs} 
\usepackage{xcolor}
\usepackage{float}
\usepackage{array}
\usepackage{placeins}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{gensymb}
	\doublespacing
	
	
	\usepackage{pdflscape}
\usepackage{everypage}
\usepackage{lipsum}

	\newcommand{\Lpagenumber}{\ifdim\textwidth=\linewidth\else\bgroup
  \dimendef\margin=0 %use \margin instead of \dimen0
  \ifodd\value{page}\margin=\oddsidemargin
  \else\margin=\evensidemargin
  \fi
  \raisebox{\dimexpr -\topmargin-\headheight-\headsep-0.5\linewidth}[0pt][0pt]{%
    \rlap{\hspace{\dimexpr \margin+\textheight+\footskip}%
    \llap{\rotatebox{90}{\thepage}}}}%
\egroup\fi}
\AddEverypageHook{\Lpagenumber}%
	
\definecolor{green}{rgb}{0,1,0}
\newcommand{\NS}[1] {{\textcolor{green}{#1}}}
\newcommand{\TM}[1] {{\textcolor{orange}{#1}}}
\newcommand{\AT}[1] {{\textcolor{blue}{#1}}}

	\begin{document}
	



\graphicspath{{./ADDM}} 

\section{Introduction} \label{chap1intro}



While there is ample evidence that our decisions are profoundly affected by the decision context, how exactly the perceived subjective value of an option changes as a function of choice environment is still a matter of debate within the literature. 

Over the past decades, several theoretical models of context-dependent valuation have been proposed. One of the earliest of these is Parducci's range-freguency theory (RFT; \citeNP{Parducci1963}, \citeyearNP{Parducci1965}), a highly influential theory in psychophysics, which offers an account of how objective stimuli get translated into subjective quantities. According to RFT, the perceived magnitude of the stimulus depends on two components, its range and rank value within the stimuli set. The range value of the stimulus reflects its position with respect to the highest and lowest stimuli in the set, whereas the rank value reflects its position within the ordered set of stimuli. RFT has proven to be an excellent explanatory framework for context effects observed in a range of domains, such as strategic decision making \cite{Vlaev2006a}, price judgments \cite{Niedrich2009}, and even pain perception \cite{Watkinson2013}.  


The range and rank principles have also been influential outside the RFT framework. For example, the effects of stimulus range on discrimination performance and magnitude judgments have long been the focus of perceptual psychology \cite{Lockhead1986}. More recently, range effects have been increasingly incorporated into models emerging from the interdisciplinary field of decision neuroscience. These models mostly focus on explaining patterns of neural activity during decision-making tasks in fMRI experiments. Given that neural firing rates are bounded from above due to biophysical constraints, it is a natural assumption that some sort of adaptive mechanism must take place to accommodate differences in the range of stimulus values that can be experienced in the real-world (e.g., \citeNP{Padoa-Schioppa2009}; \citeNP{Soltani2012}; \citeNP{Rangel2012}; \citeNP{Louie2013}).

%Value normalization in decision making: theory and evidence

The rank principle has also been successfully applied in a range of choice domains. For example, in an fMRI experiment where participants were shown pictures of monetary amounts they could win, \citeA{Mullett2013} have found that activity in certain brain regions reflected the current stimulus' rank position within the entire set of stimuli. In a socioeconomic context, several studies have shown that the rank position of one's income within their respective social reference group (e.g., workplace, neighbourhood), but not the absolute level of income, is a significant predictor of a series of health outcomes including mental health, self-reported happiness, and job satisfaction (e.g., \citeNP{Clark2008}; \citeNP{Brown2008a}; \citeNP{Boyce2010}; \citeNP{Daly2015}).

The range and rank principles are two distinct approaches to value normalisation, where the transformed values are required to fall between 0 and 1. These can be contrasted with simpler forms of normalisation, where the values are divided by the maximum value in the set, which serves as a natural reference point for a subjective valuation scale. In the context of a choice experiment, this maximum can be represented by the highest value in the current choice trial (the local maximum), or, equally, it can be the highest value experienced during the entire experiment (the global maximum), which is equivalent to simply normalising all values by a constant.

In this research project, our primary aim was to contrast these four different forms of context dependence (range, rank, local and global maximum normalisation) on the basis of their explanatory power. Our empirical strategy was to present participants with choice triplets in two experiments, in the form of complex preferential stimuli in Experiment 1, and perceptual stimuli in Experiment 2. More importantly, the ``objective value''  of each of the three options in each trial could be quantified in both experiments, which allowed us to investigate how these four value transformation rules fare in predicting choice behaviour.

There exists a wide range of choice models in cognitive psychology, many of which could serve as equally suitable theoretical frameworks to investigate context-dependence in value-based choice. In this research project, we chose to employ the theoretical framework of a hugely influential cognitive model of choice, the drift diffusion model (DDM; \citeNP{Ratcliff1978b}) and one of its popular extensions, the attentional drift diffusion model (aDDM; \citeNP{Krajbich2010}; \citeNP{Krajbich2011}), which incorporates the role of eye-movements within the standard DDM. 

Considering a trinary choice context, our empirical strategy can be described as follows. First, we identified four choice set manipulation rules (some modification of the three items' values) for which these four value transformation rules (range, rank, local and global maximum normalisation) yielded different predictions regarding choice behaviour. Second, using data from a preferential choice experiment (Experiment 1), where the selected choice sets were derived from the choice set manipulation rules, we obtained the best fitting parameters for each participant and value transformation rule from fitting the aDDM to the choice and reaction time data. Using these parameter sets, we then compared the resulting log-likelihoods to identify the value transformation rule with the highest explanatory power for each participant. Finally, using data from the perceptual version of Experiment 1, Experiment 2, we derived the best fitting parameters for each value transformation rule from fitting the DDM to group-level choice data, and conducted a qualitative comparison of the four value transformation rules' overall ability to predict changes in the choice proportions across the four choice set manipulation rules. The next section gives a brief overview of the DDM and its extension, the aDDM.


%Using data from Experiment 1, we contrasted the four value transformation rules by fitting the aDDM with each transformation rule variant to the choice and reaction time data, and compared the resulting likelihoods. Following this test, we also conducted a further comparison of the four rules, by contrasting choice proportions from Experiment 1 and 2 with our predictions from simulating out the DDM with each rule variant. The next section gives a brief overview of the DDM and its extension, the aDDM.



\section{The drift diffusion model} \label{chap1addmexplain}


How do people choose when facing multiple options? What happens exactly during the choice process, what is the cognitive mechanism underlying the comparison of the alternatives? These are the questions process models of choice seek to answer. In cognitive psychology, one particularly influential type of process models is the family of sequential sampling models. 

The overarching idea behind these models is that the evolution of preference in a given choice process is the result of noisy accumulation of evidence for each alternative throughout the decision process (hence the name “sequential sampling”), and a response is made when the accumulated evidence exceeds a certain threshold. Within the broader family of sequential sampling models, there is a wide variety of models depending on whether the evidence is accumulated separately and independently for each option and whether the threshold is defined to be absolute or relative (\citeNP{Teodorescu2013}; \citeNP{Forstmann2016}).

Of particular interest in psychology and cognitive neuroscience is a sequential sampling model that assumes a relative decision rule, and a separate and independent accumulation process for each choice option under consideration. In discrete time, this is modelled as a random walk process, whereas in continuous time it can be characterised by a diffusion process. The latter form is called the DDM (e.g., \citeNP{Ratcliff1998a}; \citeNP{Ratcliff2008}), which is now the workhorse sequential sampling model in cognitive psychology. 

In the drift diffusion framework, a key parameter of the model is the drift rate, which determines the average rate at which one of the thresholds are being approached during the choice process \cite{Voss2004a}. It also reflects the degree of similarity between the two options, and thus can be seen as a measure of task difficulty: when the options are very similar and the task is difficult, the process will have a low drift rate, and therefore it will take longer to reach one of the thresholds. Conversely, when the options are easily discriminable and the choice is ``easy'', the drift rate will be high, and a threshold is reached faster. Figure \ref{fig:randomwalk} illustrates one such evidence accumulation process in the drift diffusion framework.

\begin{figure}[h]
\centering
\caption{Illustration of the evidence accumulation process with options A and
B.}
\includegraphics[width=0.9\textwidth]{./c1_randomwalk.pdf}
\label{fig:randomwalk}
\end{figure}


While the DDM in psychology was first used as a model of memory retrieval \cite{Ratcliff1978b}, it has since been applied to a wide range of choice task domains, including lexical decision making \cite{Ratcliff2004}, numerosity discrimination \cite{Leite2011} and emotional processing \cite{Mueller2016}. The popularity of the DDM in these various research areas stems from a number of factors. 

First, and most importantly, the DDM both predicts reaction time distributions and choice probabilities remarkably well (\citeNP{Ratcliff1999}; \citeNP{Forstmann2016}). Second, it has been shown that manipulations of the decision task (such as changing task difficulty, accuracy motivation and reward structure of the task) correspond to expected changes in model parameters (drift rate, threshold distance and the starting point of the accumulation process respectively), indicating that the model is successful in capturing the cognitive mechanism underlying choice processes \cite{Voss2004a}. Lastly, the model is inherently intuitive: it provides an elegant and plausible description of the evolution of preferences during the deliberation phase of decision making.

In addition to providing a psychologically plausible model of choice behaviour, the drift diffusion model has also been successfully applied in the field of cognitive neuroscience to explain both high- and low-level cognitive processes \cite{Forstmann2016}. Studies measuring decision-related neural activity in monkeys have found that over the course of a motion discrimination task, the firing rates of neurons in the lateral intraparetial cortex (LIP; an area in the brain responsible for attentional and decision-related processes, e.g. \citeNP{Shadlen2008}) exhibit a pattern which closely resembles to the accumulation of noisy evidence (e.g. \citeNP{Churchland2011}). In humans, studies using functional magnetic resonance imaging (fMRI) have identified distinct areas of the brain whose activation changes following value manipulations of model parameters (through changing the task design) in the DDM (for a review see \citeNP{Mulder2014}). 

Owing to the popularity of the DDM, several extensions of the model have been proposed. One notable example is the incorporation of attentional processes in binary and trinary value-based decisions, known as the aDDM (\citeNP{Krajbich2010}; \citeNP{Krajbich2011}). Compared to the DDM, the aDDM modifies the drift rate for the accumulation process based on the pattern of visual fixation during choice, resulting in a faster accumulation process for the option that is being fixated at a given time point. The mathematical formulation of the accumulation process in the trinary case with options left, center and right, when option left is fixated is as follows:

\begin{equation} \label{Evidence1}
E_{t}^{left}=E_{t-1}^{left}+d\cdot r^{left}+\varepsilon_{t}^{left}
\end{equation}

\begin{equation} \label{Evidence2}
E_{t}^{center}=E_{t-1}^{center}+\theta\cdot d\cdot r^{center}+\varepsilon_{t}^{center}
\end{equation}


\begin{equation} \label{Evidence3}
E_{t}^{right}=E_{t-1}^{right}+\theta\cdot d\cdot r^{right}+\varepsilon_{t}^{right},
\end{equation}

where $E_{t}$ is the amount of evidence accumulated for a given option
(the value of the accumulator corresponding to an alternative) up
until time period $t$, $\theta$ is the penalty on the unattended
items that can vary between 0 and 1 ($\theta=1$ corresponding to the standard DDM model with no role of visual fixations, and $\theta=0$ corresponding to the case of maximum penalty, such that the accumulation process is maximally dependent on the visual fixations), $d$ is a constant that governs the rate of accumulation,
$r$ is the subjective value of the given option, and $\varepsilon$ is the noise
in the accumulation process. Therefore, for a given option, the total
amount of accumulated evidence in each time period is given by the
sum of the value of that accumulator in the previous time period,
the value of the option discounted by the penalty on the unattended
item (if it is not currently attended), and the noise component $\varepsilon\sim N(0,\sigma^{2})$ (see Equations \ref{Evidence1}-\ref{Evidence3}). The relative evidence accumulated for each option is then defined as follows:

\begin{equation} \label{REA1}
V_{t}^{left}=E_{t}^{left}-max(E_{t}^{center},E_{t}^{right})
\end{equation}

\begin{equation} \label{REA2}
V_{t}^{center}=E_{t}^{center}-max(E_{t}^{left},E_{t}^{right})
\end{equation}


\begin{equation} \label{REA3}
V_{t}^{right}=E_{t}^{right}-max(E_{t}^{left},E_{t}^{center})
\end{equation}

As can be seen form Equations \ref{REA1}-\ref{REA3}, the aDDM implements a next to best decision rule: at any given time step, each item's accumulator competes with the higher out of the other two. A choice is made when one of the relative evidence accumulators reaches a given threshold. The free parameters in the aDDM are $\theta$, \textit{d} and $\sigma^{2}$. Figure \ref{fig:driftrates} illustrates an accumulation process in the aDDM framework. In general, through $\theta$, the relative decision value for the fixated item increases, while it decreases for the unfixated items. Increasing $d$, the speed of integration results in a faster accumulation process for all options, reducing reaction times. Increasing $\sigma^{2}$ results in a noisier, less deterministic accumulation process. The speed of the accumulation also depends on the value of the options, $r$. 

\begin{figure}
\captionsetup{justification=centering}
\caption{Illustration of the evidence accumulation process in the aDDM framework
($\mathit{\mathit{r^{left}=4, r^{center}=3, r^{right}=6, d=0.0002}, \theta=0.3, \sigma^{2}=0.001}$)}
\includegraphics[width=1\textwidth]{./c1_driftrates.pdf}
\label{fig:driftrates}
\end{figure}

 Several studies have demonstrated the strong link between eye movements and subsequent choice behaviour. In a preferential choice experiment with pairs of faces used as stimuli, \citeA{Shimojo2003} have found that participants were more likely to fixate on the item they ended up choosing, a phenomenon called the gaze bias, which has also been observed in other eye tracking experiments involving preferential choice tasks (e.g. \citeNP{Armel2008}; \citeNP{Bird2012}). This effect tends to be especially pronounced towards the end of the decision: when retrospectively plotting how attention changes seconds before the choice was made, there is a much higher likelihood of the finally fixated item being the finally chosen one too (compared to the beginning of the trial). This is called the gaze cascade effect (e.g. \citeNP{Shimojo2003}; \citeNP{Glaholt2009}) or the late onset bias \cite{Mullett2016}. \citeA{Shimojo2003} argued that this effect is a result of a feedback loop between preference and visual orienting behaviour. 

While these patterns provide strong evidence for the intimate link between attention and choice behaviour, the assumption that eye movements have a casual effect on preference formation has proven difficult to demonstrate. This difficulty stems from the fact that designing an eye-tracking experiment that mimics real-life choice scenarios, whilst allowing for exogenous manipulations of gaze in a non-invasive fashion is extremely challenging from a methodological point of view. Attempts to overcome this difficulty included explicitly controlling exposure time by displaying only one item at a time \cite{Armel2008}, instructing participants to fixate on the item indicated with a specifically coloured frame in a binary choice task \cite{Lim2011}, and prompting a decision once the target has been fixated for a pre-determined amount of time \cite{Parnamets2015}. While these studies have demonstrated that items that were fixated longer were also more likely to be chosen, the artificial nature of these choice scenarios mean that they do not provide unequivocal evidence that eye movements have a causal effect on preferences in real-life choice scenarios. 

However, in a recent study, \citeA{Gwinn2019} directly tested the causal effect of eye movements on preferences. They used probability cueing, an elegant, non-invasive attentional learning technique, which induces attentional biases that spill over to a subsequent choice task. Their results from four studies lent support to the idea that attention has a causal effect on choice behaviour, providing the strongest evidence for this relationship yet.

While the causal role of fixations in choice behaviour is not a critical assumption in the aDDM framework \cite{Krajbich2019}, the model can accommodate both the gaze bias and gaze cascade effect. First, fixating on one item for longer means that the amount of relative evidence accumulated in favour of this item will be higher, therefore it is more likely to be finally chosen (gaze bias effect). Second, because evidence accumulation is faster when an item is being fixated, it is more likely that the item's relative evidence accumulator reaches the threshold during a fixation on that item (gaze cascade effect). A comprehensive investigation of six eye-tracking studies found that attention amplifies the underlying value of the option under consideration, as opposed to providing a value-independent boost in the form of a constant added to the accumulation equation, supporting the multiplicative formulation of the aDDM \cite{Smith2019}. In addition, comparisons of aDDM predictions and actual eye-tracking data have shown that the aDDM provides a remarkably good fit to the fixation, reaction time and choice data (\citeNP{Krajbich2010}; \citeNP{Krajbich2011}). Owing to its high explanatory power, we decided to investigate context-dependency within the aDDM framework. 

When fitting the aDDM to actual choice data, the value of the options (and consequently the drift rate) is usually defined as the item ratings given by participants in a ratings task that precedes the choice task ($r^{left}$, $r^{middle}$, and $r^{right}$ in Equations \ref{Evidence1}-\ref{Evidence3}). These ratings are assumed to reflect the “objective value” of an option, evaluated independently from the other options available. In this research project, we aim to incorporate the idea of context-dependency within the aDDM framework through the item ratings. In particular, we applied the four value transformation rules (range, rank, local, global maximum) to the preference ratings to accommodate and compare these rules within the aDDM framework.


These modifications reflect changes in how the collected evidence samples are defined within the model framework. In the original model it is the absolute value of the options (preference ratings on a scale from -10 to 10; e.g., \citeNP{Krajbich2011}) that governs the drift rate, whereas in our formulation, it is the normalised, subjective values of the options that feed into the accumulation equation at each time step. Importantly, we did not explicitly modify how the options are contrasted and integrated within the model (described in Equations \ref{REA1}-\ref{REA3}).



\section{Overview of Experiment 1 and 2}

In the following two experiments, our goal was to capture the idea of context sensitivity within the aDDM framework and probe the robustness of our findings. Experiment 1 was a preferential choice experiment, where participants were presented with choice triplets created from 100 movie posters, while their eye movements were recorded. In a separate rating stage that preceded the choice task, we obtained independent preference ratings from each participant on our set of 100 movies. Using these ratings as inputs to the four value transformation rules, we investigated how preferences change as a function of the choice context. In Experiment 2, we were interested in the same question, but we used perceptual stimuli, in the form of rapidly updating sequences of numbers drawn from a normal distribution with a fixed mean (taken from \citeNP{Tsetsos2012a}). Analogously, we used these fixed means as inputs to the value transformation rules.


For a thorough investigation of the explanatory power of the four value transformation rules, we used two fundamentally different methodological approaches. First, using data on eye movements during choice from Experiment 1, we obtained the best fitting set of aDDM model parameters for each valuation rule and participant, and compared the resulting log-likelihoods across the four value transformation rules. In order to obtain these best fitting parameters, we used two alternative methods to estimate the choice probabilities: a traditional simulations-based method, and a novel probability distribution method, which circumvents the problem arising from simulating out a stochastic model. Second, using a DDM simulations approach, we compared the choice proportion predictions for each value transformation rule with the results from Experiment 1 and 2, which served as a further test of the explanatory performance of each rule. 

This way, our contribution is two-fold. First, we conduct a rigorous comparison of the performance of the four subjective transformation rules, delivering insights about context-dependent choice behaviour across multiple stimuli domains. Second, we develop a novel probability density evolution approach to fitting the aDDM to the data. 

Results from the model fitting approach using data from Experiment 1 suggest that subjective valuation mostly depends on the absolute value of the choice options, and, albeit to a lesser extent, also reflects  relative value sensitivity, a finding which was supported by the results from Experiment 2  as well. We discuss these results in light of findings from neuroeconomic studies investigating the neural correlates of value-based choice.

\section{Subjective Transformation Rules} \label{chap1subjtrexplained}

We considered the four value transformation rules detailed in Section \ref{chap1intro}: the range, rank, local, and global maximum rule. Each rule takes in three “objective values” $x_1, x_2, x_3$  (these are the “raw” independent preference ratings for each movie in Experiment 1, and the mean of each distribution from which the rapidly updating sequence of numbers are drawn in Experiment 2), and transforms these into normalised “subjective values” (required to fall between 0 and 1). 

 \textbf{Range Rule.} The range value reflects the stimulus' perceived distance from the stimulus with the lowest value as a proportion of the overall distance between the highest and lowest valued stimulus in the stimuli set. It captures the idea that valuation is sensitive to the highest and lowest value encountered (or, equivalently, to the overall range of values) in a given choice set. Formally, this can be described by the following equation:
 
\begin{equation}
Range_{i}=\frac{x_i - min(x_1,x_2,x_3)}{max(x_1,x_2,x_3)-min(x_1,x_2,x_3)}
\label{eq:range}
\end{equation}
 Note that the items with the lowest and highest original values are always assigned 0 and 1 respectively, while the middle item's value can vary between 0 and 1.
 
 
 \textbf{Rank Rule.} The rank rule also follows from RFT, and reflects the “frequency” of the stimuli, which can be translated as its ranked ordinal position in the choice set. The rank rule reflects a somewhat simpler mechanism of valuation, whereby the subjective value of an item is solely determined by the number of items with lower or higher values in the choice set, and the extent of the difference between item values does not matter. Formally, given an ordered set of three stimuli $x_1, x_2, x_3$ , the rank values of the stimuli are given by
 
\begin{equation}
Rank_{i}=\frac{i-1}{n-1}
\label{eq:rank}
\end{equation}

Note that according to the rank rule, the three subjective values assigned to the items with the lowest, middle and highest value are always 0, 0.5 and 1, respectively.

\textbf{Local Maximum Rule.} The local maximum rule generates subjective value representations by dividing each stimuli's value by the highest value encountered on that trial, and in this way it corresponds to the normalised version of the raw ratings used in \citeA{Krajbich2011}. This reflects a bias towards the most rewarding, and hence potentially most salient option at the time of the choice. Formally, the subjective value of a stimuli according to the local maximum rule is given by

\begin{equation}
Local max_{i}=\frac{x_i}{max(x_1,x_2,x_3)}
\label{eq:locmax}
\end{equation}

Note that the stimuli with the highest value is always assigned a subjective value of 1, while the other two values can vary between 0 and 1 (exclusive).

\textbf{Global Maximum Rule.} The global maximum rule is identical to the Local Maximum Rule, except that the objective values are normalised using the highest value encountered in the whole of the experiment (and not just on that trial). This means that all values are normalised by the same constant, therefore, the global maximum rule is equivalent to the original version of the aDDM (e.g., \citeNP{Krajbich2011}), and can serve as a natural reference point to test whether the explanatory power of the model can be increased with a different value transformation rule. Formally, in an experiment where the items that can be encountered are $x_1, x_2, ..., x_n$, the subjective value of a stimuli set according to the global maximum rule is given by

\begin{equation}
Globalmax_{i}=\frac{x_i}{max(x_1,x_2,...,x_n)}
\label{eq:globmax}
\end{equation}

Note that when an item in the current trial has the highest value in the whole of the experiment, this rule is equivalent to the local maximum rule, otherwise the values can vary between 0 and 1 (exclusive). 

\section{Choice Set Manipulations} \label{chap1choicesetman}

After identifying the four value transformation rules, the next step was to find a set of choice set manipulations for which these transformation rules (accommodated within an aDDM or DDM framework) produce different predictions about choice behaviour. A choice set manipulation represents some modification of the three options' objective values based on a certain rule (e.g. doubling each value, or adding a constant to each of them). The key idea is that the four subjective value transformation rules (range, rank, local and global maximum) differ in their predictions about how these choice set manipulations affect choice proportions, which allowed us to assess their relative empirical performance. 

Based on a series of simulations, we chose to investigate the effect of the following four distinct choice set manipulations: adding the same constant to each of the three values, multiplying each value by a constant, and having a distant or a close second, and third value. Figure \ref{fig:choicesetmanip} shows the subjective values (these are required to fall between 0 and 1) derived from each transformation, assuming a raw value scale from 1 to 7. The orange dots are the original values, while the purple dots show the values after the transformation (for example, in the case of adding a constant, the original value set is 1,2,4, while the new values are 4,5,7).

\begin{figure}
\captionsetup{justification=centering}
\centering
\caption{Subjective values by choice set manipulation and value transformation rule. The four columns show each choice set manipulation, while the rows correspond to the four value transformation rules.}
\includegraphics[width=1\textwidth]{./explain.pdf}
\label{fig:choicesetmanip}
\end{figure}

These differences in the subjective values translate into differences in choice behaviour. When adding a constant to a set of values, the range and rank transformed values are unaffected, while the global maximum transformed values shift by a constant, therefore these three rules predict no change in choice behaviour. However, the local maximum rule predicts that the distance between the best and the other two values narrows, thereby reducing the relative choice share of the best option. Similarly, the range, rank and local maximum rule predict no change when values are multiplied by a constant, whereas the global maximum rule predicts that the best option becomes relatively more attractive compared to the other two. When increasing the value of the middle option (distant vs close second), the range and both maximum rules predict that relative share of the middle option increases, in contrast with the rank rule that predicts no change. Finally, the range rule and the two maximum rules predict that increasing the value of the worst option (distant vs close third) is going to increase or decrease the share of the best option, respectively, while the rank rule predicts no change in the relative choice proportions.


Our original plan was to present participants with these seven distinct trinary choice sets (124, 457, 123, 246, 137, 167, 567) to maximise the discriminatory power of our experiments. However, due to an experimenter error, in Experiment 1, the displayed choice options were quasi-random, resulting in 84 unique choice sets instead of the originally planned seven. We nevertheless proceeded with fitting the aDDM to the data from Experiment 1, and had enough data for the relevant seven choice sets to conduct a further comparison of choice behaviour in Experiment 1 and 2.


\section{Experiment 1}

\subsection{Method} \label{chap1exp1method}

Experiment 1 was a preferential choice experiment, where participants' eye movements were recorded during choice. Based on previous eye-tracking studies (e.g., \citeNP{Krajbich2010}; \citeNP{Krajbich2011}), we decided in advance that a sample of 50 participants should provide enough statistical power for our purposes. To reach this sample size, we recruited sixty participants overall, out of which ten participants could not be eye-tracked. Participants were recruited through the University of Warwick's Research Participation System, and were paid £7.00 for taking part in the experiment. We did not record data on participants' gender, as we did not expect it to affect the results. Ethical approval was obtained from the Department of Psychology, University of Warwick.

The experiment consisted of two parts. First, participants who signed up for the study were required to complete an online questionnaire, where they had to rate 110 movies on a scale from 1 to 7, based on how much they liked the given movie (1 being not at all and 7 being very much so). We also asked them whether they have seen the movies before. The rating task began with 10 test trials (these were the same movies for all participants), so that participants could familiarise themselves with the task and the range of movies they will be required to rate. Then, they were informed that the actual rating task is about to start. During this task, participants were presented with the 100 movies in a randomised order, and we instructed them to try to use the whole range of the scale as much as possible. This was important, because the second part of the experiment relied on these ratings, and could not be run without at least one movie for each rating. Some participants failed to do this, and so they were asked again to complete this task before coming to the lab for the second part of the experiment. Figure \ref{fig:ratingstask} shows an example trial from this task, which took about 15 minutes on average. 

\begin{figure}[htb!]
\captionsetup{justification=centering}
\centering
\caption{Example rating task from the first part of Experiment 1. }
\includegraphics[width=0.58\textwidth]{./ratingstask.png}
\label{fig:ratingstask}
\end{figure}


Those who successfully completed the first part were invited to the lab for the second part of the study. In the second part, participants were presented with 100 trinary sets of the pre-rated movies, and were asked to pick the one they liked the most on each trial by pressing the corresponding keyboard key (left, down, right arrow), while their eye movements were recorded at 500 Hz using an EyeLink 1000 (SR Research). The order of the choice triplets and the display order of the three movies in a given trial were both randomised. The eye tracker was calibrated after every 25 choices (four times overall over the course of the experiment), and each trial began only after participants fixated on a centrally presented fixation cross for 500 ms. We did not add a text prompt to the screen to avoid additional visual distractions. Figure \ref{fig:exp1_movieschoice} shows an example trial from the second part of this experiment.

\begin{figure}[htb!]
\captionsetup{justification=centering}
\centering
\caption{Example choice task from the second part of Experiment 1. }
\includegraphics[width=0.7\textwidth]{./exp1_movieschoice.jpg}
\label{fig:exp1_movieschoice}
\end{figure}



\subsection{Stimuli}

We chose 110 movies that received the most votes in 10 distinct genres (adventure, comedy, drama, family, fantasy, history, mystery, romance, sci-fi, thriller) on IMDb as of March, 2016. In the first part of the experiment, the first ten trials were practice trials (one movie from each genre), which aimed to give participants an idea about the range and type of movies they will be asked to rate. 

In the second part of the study, the displayed trinary sets were originally generated based on the participant's ratings to reflect the choice set manipulations. However, as mentioned above, the choice sets displayed during the experiment were unintentionally quasi-random, due to the incorrect numbering of the 110 stimuli pictures. Each number still corresponded to a unique movie (albeit this assignment was random), but the choice sets, which were based on the movie numbers, were displayed as planned. This resulted in 84 unique trinary choice sets, defined by the raw rating of each of the three options, as opposed to the originally planned seven, described in section \ref{chap1choicesetman}. 

In order to avoid the same set of movies appearing again, the script that generated the choice triplets was written to ensure that all the stimuli with a given rating gets used before any repetition of stimuli occurs. While we tried to vary the stimuli appearing as much as we could, repetition was inevitable in some cases, since some of the participants gave very uneven ratings (e.g., gave a rating of 1 to only a few movies). 

Given that our main aim was to fit the aDDM to the data to compare each value transformation variant, we decided it was still worth proceeding with the analysis with the current dataset (even though the data now contained considerably fewer trials that could help us delineate the differences in the explanatory power of the four value transformation rules). 

\subsection{Exclusion criteria}

As mentioned in section  \ref{chap1exp1method}, we had usable eye-tracking data from 50 participants. For a few participants, the calibration quality was not satisfactory in some parts of the experiment, and therefore we excluded these trials (accounting for about 2.5\% of all fixations). We also excluded the first three trials of each participant, and each trial right after a calibration, to make sure we only include fixations that are part of the choice process. We then excluded fixations that fell outside the three areas of interest, and trials in the upper 1\% of the reaction time distribution. Finally, we only kept trials where all three of the choice items were fixated at least once. After applying these exclusion criteria, the average number of trials per participant was 80.


\section{Fitting the aDDM}

Using data from Experiment 1, we wished to compare the four value transformation rules' explanatory power within the aDDM framework, on the basis of their respective log-likelihoods of producing each participant's data. To do this, we needed to establish a method for deriving choice probabilities, given a specific set of parameters. We used two fundamentally different methodological approaches to derive these probabilities. Below we first describe the standard model fitting approach used in the aDDM literature, and then we highlight how our empirical strategy constitutes an improvement compared to this standard approach.


When fitting the aDDM, the common approach is to fit the model on the group-level, and simulate out the choice process for all parameter value set ($\theta$, the penalty on the unattended item, $\sigma$, the noise parameter, and $d$, the speed of integration) and preference rating combinations, with fixations sampled from the empirical distribution of fixations, conditional on the preference rating difference  (e.g., \citeNP{Krajbich2010}; \citeNP{Krajbich2011}; \citeNP{Fisher2017}). 

Then, given a discrete number of reaction time bins, the number of simulations falling into each choice and reaction time bin can be calculated for all parameter and rating difference combinations, and the probability that a simulation trial finishes in a given choice and reaction time bin can be derived. To obtain the log-likelihood of the data for each parameter combination, these probabilities are multiplied by the number of empirical data points falling into the same choice and reaction time bin, and their logarithm is taken and summed up. One or two iterative grid searches can be carried out using this estimation method, resulting in the final estimate of the best fitting parameter values, and the corresponding log-likelihood.


Our empirical approach differs from this method in several respects. First, in contrast with their group-level approach, we derived the best fitting parameters for each subject to account for individual heterogeneity. Therefore, our primary aim was to find the best fitting parameter set and the corresponding log-likelihood value for each participant and transformation rule (of which there were 200, given 50 participants and four subjective value transformation rules) to determine which transformation rule yields the best fit for each participant. Second, to derive the most accurate estimate of the predicted probability of each trial falling into a given choice and reaction time bin, we used the eye movement pattern on that very trial (as opposed to probabilistically sampling it from an empirical distribution, conditional on the rating difference). Third, in addition to the grid search, we also used a Nelder-Mead optimization algorithm to find the best fitting parameter values for each participant and value transformation rule. Finally, we used 100,000 simulations to lessen the effect of noise (an inherent feature of the aDDM) and obtain more reliable choice probability estimates, whereas previous studies used between 1000 and 4000 simulations (e.g., \citeNP{Krajbich2011}; \citeNP{Fisher2017}).  

In addition to these improvements, to evaluate the robustness of our estimates as well as to circumvent a methodological issue stemming from applying a traditional simulations-based approach to a stochastic model, we also used a novel probability distribution-based approach to derive the best fitting parameter combinations and corresponding log-likelihoods for each participant and value transformation rule.

Before the parameter estimation, we discretized the eye movement data. To do so, we divided each trial into 200 ms bins, where each bin was one step in the evidence accumulation process assumed by aDDM. The fixated item in a bin was sampled probabilistically, based on the proportion of time spent on fixating on a given item in each bin. For example, if in one bin, 50 ms was spent on fixating on item 1, and 150 ms was spent on fixating on item 2, then in the simulations item 1 had a 25\%, while item 2 had a 75\% probability of being chosen as the fixated item in that bin. 

When fitting the aDDM for each value transformation rule, we used the transformed ratings in place of $r$ described in Equations \ref{Evidence1}-\ref{Evidence3} in section \ref{chap1addmexplain}. However, the unintentionally wide variety of value triplets displayed in Experiment 1 posed a question about how to derive the range transformed values in case there were two or three identical values in the choice set (it is straightforward in the case of the two maximum rules and the rank rule). We decided to code the transformed range values as 1 if all values were identical. If there were two equal values in a triplet, they were either coded as 1 or 0 (depending on whether the third value was lower or higher than the two identical ones, respectively). 


\subsection{Simulations method}

To summarise the main novel aspects of our stochastic simulation approach, we model individual trials from individual participants (as opposed to deriving group-level parameter estimates), and use the eye movements specific to the very trial we are modelling (as opposed to randomly drawing it from the distribution of fixations from all trials). In addition, we find the best fitting parameter estimates through an optimization algorithm, and all probability estimates are based on 100,000 simulations of each trial.

To obtain the relevant parameter sets for each participant and value transformation rule, we first simulated out each trial, and calculated the probability that the eventually chosen item was selected in the correct time bin. Note that there are two sources of variation in these simulations: the inherent noise in the accumulation process (captured by $\sigma$), and the non-deterministic fixation pattern arising from the transformation of the fixation data into time bins. 


To find the best fitting parameter sets, we started off with a grid search with the following values: $\theta=\{0.2, 0.45, 0.6, 0.85, 1\}$, $\sigma=\{0.1, 0.14, 0.2, 0.28, 0.4\}$, $\textit{d}=\{0.1, 0.17, 0.31, 0.56, 1\}$, 125 parameter sets overall, for each participant and value transformation rule. These parameter values were chosen after an iterative process, ensuring that most participants' best fitting values fell in the middle of the range of the possible grid values. This was done to make sure that the algorithm does not have to move considerable lengths in the parameter space to find the best fitting parameter set in the subsequent optimization process.


Upon obtaining the best fitting parameter set from the grid, we started a Nelder-Mead optimization algorithm with a maximum iteration number of 200, using the resulting parameter set from the grid search as starting points for each participant and value transformation rule. The same process was repeated once more using the results from the first Nelder-Mead optimization process as a starting point, to increase the probability that we find the parameter set that results in the highest log-likelihood of producing the data. 

However, there is a methodological difficulty arising from the stochastic nature of the aDDM. Due to the inherent noise in the process, two runs of simulations of the same trial with the same parameter values naturally gives slightly different results. This is problematic if the estimated probability of the trial is generally low (near zero), as it can happen that in one simulation run, the estimated probability is a positive number, while in the next one it is zero. When taking the logarithm of all trial probabilities and summing them up, having just one trial with a zero probability results in a log-likelihood value of minus infinity. At the next simulation attempt, it is possible that the estimated probability for the same trial will be positive this time, and depending on the estimated probability of the rest of the trials, this can in turn result in a relatively high overall log-likelihood value. Such dramatic changes in the log-likelihood values can easily lead to the Nelder-Mead process getting stuck at a given point in the parameter space.

This is because the Nelder-Mead optimization method was initially developed for deterministic models, and it has been demonstrated that substantial noise in the underlying model can lead to false convergence (e.g., \citeNP{Chang2012}; \citeNP{BartonIvey1991}). While running the second Nelder-Mead did not lead to substantially different parameter estimates for most participants and value transformation rules, we still wanted to alleviate concerns about the validity of the simulations method, and decided to compare the results from the simulations approach with those obtained from an alternative, deterministic implementation of the aDDM.


\subsection{Probability distribution method}

To circumvent the problem arising from the stochastic nature of the aDDM, we utilised an alternative estimation method that is free of stochasticity. Rather than deriving the probability of the data through noisy simulation, we calculate it directly, by using a numerical evidence integration technique. This approach removes the problem of variable log-likelihood values, thus making the parameter search much easier.


 In short, our approach was to derive the probability distribution of the accumulation process over the relative evidence states ($E_{1}-E_{2}$ and $E_{1}-E_{3}$ from Equations \ref{Evidence1}-\ref{Evidence3} in section \ref{chap1addmexplain}, with $left, center, right$ changed to 1, 2, 3, respectively), for each time bin and parameter combination. This method allowed us to directly calculate the exact choice probability for each trial in our data. Our approach is conceptually equivalent to that used by \citeA{markovchain}, who approximated the diffusion process using Markov Chain Theory, and derived the transition probabilities between different evidence states. However, we apply this method to a more complex setting than described in \citeA{markovchain}, in that implementing the aDDM entails a trinary choice case, with variable drift rates within a trial (stemming from changes in attention). \AT{Is this ok?}

We chose to base our calculations on a ``best versus average of other two''  termination rule:

\begin{equation} \label{REA1pd}
V_{t}^{1}=E_{t}^{1}-\frac{E_{t}^{2}+E_{t}^{3}}{2}
\end{equation}

\begin{equation} \label{REA2pd}
V_{t}^{2}=E_{t}^{2}-\frac{E_{t}^{1}+E_{t}^{3}}{2}
\end{equation}


\begin{equation} \label{REA3pd}
V_{t}^{3}=E_{t}^{3}-\frac{E_{t}^{1}+E_{t}^{2}}{2}
\end{equation}

as opposed to the original ``best versus next best'' formulation seen in Equations \ref{REA1}-\ref{REA3} in section \ref{chap1addmexplain}. However, we did not expect this modification to have a dramatic effect on our results, since previous research had shown that the two rules yield very similar predictions (\citeNP{Krajbich2011}). The reason why we preferred the ``best versus average of other two'' rule over the ``best versus next best'' rule is because it results in a bounded area of the finished evidence states (corresponding to combinations of $E_{1}$, $E_{2}$ and $E_{3}$, where the threshold had been reached, and one of the options had been chosen), defined in terms of the two relative evidence states ($E_{1}-E_{2}$ and $E_{1}-E_{3}$). This is illustrated by Figure \ref{fig:rulesfinished}.

\begin{figure}[htp!]
\captionsetup{justification=centering}
\centering
\caption{Finished evidence states with Next best and Average of other two termination rules.}
\includegraphics[width=0.9\textwidth]{./rulesfinished.png}
\label{fig:rulesfinished}
\end{figure}

As mentioned, we wished to calculate the probability distribution of the accumulation process over the relative evidence states $E_{1}-E_{2}$ and $E_{1}-E_{3}$ (each of which is represented by a dot on Figure \ref{fig:rulesfinished}), for a given parameter set and time step. Assuming that each step in the process can be described by a two-dimensional vector with components
\begin{equation}
\begin{array}{l}
\displaystyle x = E_{1}-E_{2} = d(\theta_{t}r^1-\theta_{t}r^2) + (\varepsilon_{t}^{1}-\varepsilon_{t}^{2})\\
\displaystyle y = E_{1}-E_{3} = d(\theta_{t}r^1-\theta_{t}r^3) + (\varepsilon_{t}^{1}-\varepsilon_{t}^{3}),\\
\end{array} 
\label{eq:axes1}
\end{equation}

the covariance matrix between the two axes is given by

\begin{equation}
\begin{array}{l}
COV = \begin{bmatrix}
       2\sigma^2 & \sigma^2 \\[0.3em]
       \sigma^2 & 2\sigma^2 \\[0.3em]
     \end{bmatrix}
\end{array} 
\label{eq:covariance}
\end{equation}

This covariance matrix is not diagonal, because the axes are correlated (due to the shared component $\varepsilon_{t}^{1}$ in the equations defining the relative evidence states), which, when modelling the process, results in an ellipse whose axes are not parallel to the coordinate axes (see the left panel on Figure \ref{fig:rotate}).  To eliminate the correlation between the axes, we can place them at a more convenient position, by rotating both counterclockwise until the ellipse axes are parallel to the coordinate axes (as demonstrated in the right panel of Figure \ref{fig:rotate}). 
%This could be improved

\begin{figure}[htp!]
\captionsetup{justification=centering}
\centering
\caption{Demonstration of the correlation problem. The left panel shows the original axes, and the right panel shows the rotated axes with no correlation.}
\includegraphics[width=1\textwidth]{./rotate.png}
\label{fig:rotate}
\end{figure}
 
In this rotated coordinate system, the probability distribution over the relative evidence states can be characterised by a bivariate normal distribution of two independent variables, allowing us to substantially simplify the subsequent calculations by relying on the normality of the joint distribution of the two relative evidence states. The rotated axes, $x'$ and $y'$ can be characterised \cite{algebra} as
\begin{equation}
\begin{array}{l}
\displaystyle x' = x cos(\alpha) - y sin(\alpha)\\
\displaystyle y' = x sin(\alpha) + y cos(\alpha).\\
\end{array} 
\label{eq:axes2}
\end{equation}

In Figure \ref{fig:rotate}, the angle of rotation is $\alpha = 45\degree$. Expressed in terms of our original relative evidence states, this $45\degree$ degree anti-clockwise rotation means that the new axes can be defined as

\begin{equation}
\begin{array}{l}
\displaystyle x' = E_{1}-\frac{E_{2}+E_{3}}{2}\\
\displaystyle y' = \frac{E_{2}-E_{3}}{2}.\\
\end{array} 
\label{eq:axes3}
\end{equation}
 
As the right panel in Figure \ref{fig:rotate} demonstrates, this results in an ellipse whose axes are now parallel to the coordinate axes. In this transformed coordinate system, we can characterise the decision process as a multivariate normal distribution, whose movement along the relative evidence bins is governed by the model parameters. To save computation time, the possible evidence states were approximated by bins (the x evidence space spanned from -2.5 to 1.5 with spacing 0.1 in 41 bins, whereas the y evidence space spanned from -1.5 to 1.5 with spacing 0.1 in 31 bins, resulting in $31\cdot41 = 1271$ bins overall). Our aim was to calculate the probability distribution of the diffusion process over this grid for each time period in the decision process, which allowed us to directly derive the probability that an item is chosen at any given time point (by summing up the probability mass over the evidence bins in which the item was chosen, see the finished evidence states in Figure \ref{fig:rulesfinished}).

Figure \ref{fig:process} illustrates the diffusion process. On each trial, at time step 0, the process starts from the central bin defined by $x = 0$ and $y = 0$, with probability density 1, corresponding to no evidence for any of the three alternatives. As shown in the upper panel of Figure \ref{fig:process}, at each subsequent time step, we can model the movement and diffusion of this probability mass over our evidence grid. For example, in this example, in the first two time steps, Alternative 1 is fixated, and thus the overall probability mass starts moving towards the east (approaching the relative evidence bins that correspond to Alternative 1 being chosen). The movement of the probability mass throughout the choice process is governed by the model parameters. At each time step, we can derive the choice probability that either of alternatives are chosen by summing up the probability over the relevant evidence states. The corresponding choice probabilities are depicted in the lower panel of Figure \ref{fig:process}. Below we explain in more technical detail how the choice probabilities were derived.


\newgeometry{left=1in,bottom=1.5in, right = 1.5in, top = 1.5in}
%\begin{landscape}
%\pagestyle{empty}%
\begin{figure}
\captionsetup{justification=centering}
\caption{Illustration of the diffusion process over 8 time steps with attention pattern $1, 1, 2, 2, 2, 3, 3, 3$; $\sigma = 0.3$; $\theta = 0.67$; $d = 1$, $r = 0.5, 0.5, 0.5$}

\vspace*{12mm}
\centering
\includegraphics[width=1.1\textwidth]{process.pdf}
\label{fig:process}
\end{figure}
%\end{landscape}
\restoregeometry


From a computational perspective, we derive these probabilities through a transition matrix. In each time period, we calculate the probability distribution over the grid by multiplying the previous time period's probability mass with our transition matrix. This matrix has two components. 

The first component characterizes the diffusion of the probability density with $\sigma$ as the only input to a bivariate normal distribution with mean $\mu_x=0$ and $\mu_y=0$. Specifically, this is done by taking each of the 1271 bins in the grid as the center of a bivariate normal distribution with standard deviation $\sigma$, and calculating the exact probability mass falling into each of the 1271 bins in the grid. We can calculate this probability vector for all our bins in the grid, which gives us a $1271\cdot1271$ transition matrix, where each entry is the transition probability from one bin to another. Starting with a probability mass entirely concentrated in the bin defined as $x = 0$ and $y = 0$ at time step 0 (corresponding to no evidence for any of the alternatives), we can recursively multiply the resulting probability mass in each time step with this transition matrix. In absence of the second component, this process leads to a diffusion of the probability density around our starting point $x = 0$ and $y = 0$, corresponding to a diffusion process with no drift rate corresponding to a random walk process. 



The second component is the drift rate, defined by $\theta$, $r$ and $d$ (described in Equations \ref{Evidence1}-\ref{Evidence3} in Section \ref{chap1addmexplain}), which governs the direction of the movement of the probability mass. The drift rate can be incorporated in the transition matrix by shifting the mean of the bivariate normal distributions. Because the drift rate varies depending on the attention pattern (through $\theta$), three transition matrices can be constructed, depending on which item is fixated. Taken together, through the model parameters, these two components define the transition probabilities over the relative evidence states, and determine the movement of the bivariate distribution in our grid.  


As explained above, we can then derive the probability of choosing either of the options in at any given time period by summing up the probability mass entering the bins corresponding to the option's finished evidence states in that time step. Before moving to the next time period, all the probability mass over the finished evidence states are set to 0, allowing us to derive the probability that an option is chosen exactly in the next time period. To ensure that this estimation method is completely free of stochasticity, we used a deterministic fixation pattern, based on which item was fixated first in each 200 ms bin. 




Once we were able to calculate the choice probabilities using this method, we again started off with a grid search to find the best fitting parameters for each participant and subjective value transformation rule. We used the following parameter grid: $\theta=\{0.33, 0.67, 1\}$, $\sigma=\{0.1, 0.55, 1\}$, \textit{d}=$\{0.1, 0.55, 1\}$ in running the Nelder-Mead optimization algorithm. We used a smaller grid, because there is no stochasticity in this method, but it is also computationally more intensive to calculate (calculating the log-likelihood for one participant takes 13 times longer compared to the simulation approach). For the same reason, we only ran the Nelder-Mead process once more after the grid search, with a maximum iteration number of 100.  



\subsection{Results}

For both empirical methods (simulations and probability distribution approach), the model fitting process resulted in 200 log-likelihoods, one for each value transformation rule--participant pair. We wished to determine which value transformation rule produces the highest log-likelihood for each participant and estimation method. For each participant, the four subjective value variants of the aDDM have the same number of parameters, and are calculated on the same set of data, therefore the log-likelihoods are directly comparable. Table \ref{table:chap1res} shows the distribution of participants based on their respective best fitting value transformation rules, demonstrating that the two estimation approaches have yielded very similar results.
 
 
\begin{table}[ht]
\caption{Best fitting value transformation rule for each participant by estimation method.}
\centering
\begin{tabular}{ccccc}   

\toprule Approach  & Global Max    & Local Max & Rank & Range \\ 
\midrule Simulations  &  29 (58\%) & 11 (22\%)  & 4 (8\%)   & 6 (12\%) \\
       Probability Distribution & 28 (56\%)  &    9 (18\%)    & 8 (16\%)   & 5  (10\%)  \\
\bottomrule 
\end{tabular}
\label{table:chap1res}
\end{table}

For the majority of participants (56-58\%), the global maximum rule provided the best fit, regardless of the estimation approach. This is followed by the local maximum rule, which provided the best fit for 18-22\% of the participants. The range rule proved to be the best fitting model for only about 10-12\% of participants. Interestingly, when using the probability distribution approach, the rank rule provided the best fit for 16\% of participants, whereas the same number for the simulations approach is only 8\%. 

Table \ref{table:chap1res2} shows the median estimated parameter values by value transformation rule and estimation method. The parameter estimates do not vary considerably across value transformation rules or estimation methods. For the list of  best fitting parameter values for each participant by value transformation rule and estimation method, see Appendices \ref{chap11}-\ref{chap12}.

\begin{table}[h]
\centering
\caption{Median best fitting parameter values by value transformation rule and estimation method.}
\begin{tabular}{lcccccc}
\toprule 
\multirow{2}{*}{} & \multicolumn{3}{c}{Simulations} & \multicolumn{3}{c}{Probability} \\
\multirow{2}{*}{} & \multicolumn{3}{c}{Approach} & \multicolumn{3}{c}{Approach} \\

\cmidrule(l){2-4} \cmidrule(l){5-7}
 & $\theta$ & $\sigma$ & $d$ & $\theta$ & $\sigma$ & $d$ \\
\midrule
Global Max & 0.68 & 0.2  & 0.31 & 0.7 & 0.19 & 0.29 \\
Local Max & 0.67 & 0.2 & 0.25 & 0.68 & 0.19 & 0.24\\
Range & 0.54 & 0.2 & 0.15 & 0.56 & 0.2 & 0.14\\
Rank & 0.51 & 0.21 & 0.16 & 0.53 & 0.2 & 0.14\\
\bottomrule
\end{tabular}
\label{table:chap1res2}
\end{table}



Overall, these results point to a cognitive mechanism where subjective valuation largely depends on the absolute value of the options, and, albeit to a lesser extent, is also affected by the relative magnitude of the options (compared to the maximum value on the current trial). In addition, the almost identical results from the two estimation approaches underline the robustness of the findings, and alleviates concerns about the reliability of the simulations method.

 
 
\section{Experiment 2}

In Experiment 2, we still relied on the choice set manipulations to test the explanatory power of the four value transformation rules, but we used a fundamentally different stimuli and empirical approach. We assessed the explanatory power of the four value transformation rules by their ability to qualitatively predict how the choice set manipulations affect the choice proportions, using choice proportion data from Experiment 1 and a perceptual version of the same task (Experiment 2). 

Experiment 2 had several rationales. First and foremost, comparing the results from a further comparison of choice probabilities with the results from a model fitting approach allows us to gain a deeper understanding of the relative explanatory power of each value transformation rule. While we have some choice data on the seven relevant choice sets from Experiment 1, due to the error in the experimental procedure, it is not enough to conduct a test with appropriate statistical power. Second, it can be argued that the preference ratings participants gave in the first part of Experiment 1 are already context-dependent, since the movies are evaluated with respect to each other in the ratings task, which could have biased our previous results. Finally, since the movie stimuli are somewhat complex in the sense that there are a range of factors we could not control for (e.g. visual saliency, mnemonic processes the stimuli can induce), repeating the experiment with a simpler, and more neutral stimuli will inform us about the validity and generalisability of our results. 

\subsection{Method}

We recruited 130 participants through the University of Warwick's Research Participation System. On each trial, participants were presented with a trinary set of rapidly changing sequences of numbers (taken from \citeNP{Tsetsos2012a}) and were asked to choose the sequence with the highest average of numbers by pressing the corresponding keyboard key (left, down, right arrow). The numbers were updated every 210-250 ms. To familiarise participants with the task, the experiment began with three practice trials, where feedback were given on which sequence they chose. After the practice trials, there were 140 trials to be completed in two blocks (20 trials per choice set). Participants could take a break for as long as they wished between the two blocks, and they were paid £5.00 for taking part in the experiment. Figure \ref{fig:experi2_numbers} shows one trial from this experiment. The experiment on average took about 15 minutes to complete. Ethical approval was obtained from the Department of Psychology, University of Warwick.

\begin{figure}
\captionsetup{justification=centering}
\centering
\caption{Example trial from Experiment 2.}
\includegraphics[width=0.7\textwidth]{./experi2.png}
\label{fig:experi2_numbers}
\end{figure}

\subsection{Stimuli}

Numbers for each presented sequence were derived from a truncated normal distribution with $\sigma=18$, and a mean that was calculated from a monotonic transformation of the ratings from Experiment 1. Specifically, we multiplied each number by ten, and further added ten to them. For example, a trial with values 1,2,4 in Experiment 1 corresponded to a trial with means 20, 30, 50 in Experiment 2. The distribution was truncated to only include numbers between 0 and 99. We chose to work with the two-digit equivalents of the values from Experiment 1 to increase the range of numbers that could be sampled, and the constant was added to shift the lower end of the value distributions away from 0. When a number below 10 was sampled, it was displayed as a two-digit number with 0 as the first digit to prevent any differences in saliency between one- and two-digit numbers.


\subsection{Results}

For a further test of the four value transformation rules, we compared the empirical choice proportions (from Experiment 1 and 2) with the choice set manipulation predictions of each value transformation rule. The choice proportion predictions for each value transformation rule were derived from simulating out each trial using DDM 10,000 times. These simulations focused only on the eventually chosen item, and did not take reaction times into account.

In the DDM, there are two parameters, $\sigma$, the noise, and $d$, the speed of integration. While our main interest, the qualitative patterns of predicted choice proportions are largely insensitive to the exact parameter values, we still needed to determine the exact parameter values to use for generating predictions for each value transformation rule. To derive these (group-level) parameter sets, we again used a Nelder-Mead optimization algorithm minimising the squared difference between the simulated and empirical choice proportions for each value transformation rule.  The empirical choice proportions were based on the results from Experiment 2 as opposed to Experiment 1, due to the much larger sample size for the relevant choice sets. We ran the optimization algorithm with a maximum iteration number of 150 twice, first with starting parameters  $\sigma = 0.1, d = 0.1$, and then using the resulting parameters from the first run as starting parameters for the second optimization process. Table \ref{table:chap1res3} shows the best fitting parameter pairs for each value transformation rule. 

\begin{table}[htb!]
\centering
\caption{Best fitting group-level parameter values from the grid search for each value transformation rule.}
\begin{tabular}{lcccccc}
\toprule 
 & $\sigma$ & $d$ \\
\midrule
Global Max & 0.12 & 0.09  \\
Local Max & 0.13  & 0.07\\
Range & 0.15  & 0.06 \\
Rank & 0.15  & 0.06\\
\bottomrule
\end{tabular}
\label{table:chap1res3}
\end{table}


\FloatBarrier

%Although the qualitative patterns of predicted choice proportions are largely insensitive to the exact parameter values, for the sake of comparability, we decided to use the same parameter pair ($\sigma$ and \textit{d}) in all simulations. This parameter pair was derived in the following way.

%Using data from Experiment 1, our goal was to select the parameter pair that provided the best fit across all four value transformation rules on average. First, using a $5x5=25$ grid space with $d=\{0.01, 0.03, 0.1, 0.32, 1\}$ and $\sigma=\{0.01, 0.03, 0.1, 0.32, 1\}$,  we calculated the difference between the predicted and actual choice probabilities for each parameter pair and value transformation rule. We simulated out the DDM 100,000 times to obtain the predicted probabilities. This allowed us to rank each of the 25 parameter pairs for each value transformation rule based on how well it fits the actual data (for each rule, the parameter pair that produced the smallest difference between predicted and actual probabilities was allocated a rank of 1, while the pair that produced the highest difference receved a rank of 25). Then, using the parameter pair that had the smallest sum of ranks across the four value transformation rules, we started a Nelder-Mead optimisation process that aimed to minimise the average probability difference across all rules. We once more restarted the Nelder-Mead process using the previous parameter results, which finally produced the parameter pair $d=0.012$ and $\sigma=0.0761$.



Figures \ref{fig:addconstant_all}-\ref{fig:closedist3rd_all} show the predictions for each rule and the associated empirical choice proportions from both experiments. In these figures, the four panels to the left correspond to the choice proportion predictions from the DDM simulations, while the two panels on the right show the resulting choice proportions from the two experiments and their associated 95\% CIs. All model predictions are based on 100,000 simulations. The difference in the precision of the estimates from the two experiments reflects the difference in the amount of choice data available for the relevant choice sets.  




\textbf{Adding a constant.} Figure \ref{fig:addconstant_all} shows the effect of adding a constant. The local maximum rule predicts that this value manipulation results in the best option becoming relatively less attractive compared to the other two, whereas none of the other three value transformation rules predict any change in the choice proportions. Unfortunately, the wide confidence intervals around the choice proportions from Experiment 1 do not allow us to see any clear patterns, but the results from Experiment 2 are broadly in line with the predictions from the local maximum rule (albeit the changes are less pronounced than predicted). In summary, the results from adding a constant lend partial support to the local maximum rule, as no other value transformation rule predicts the correct qualitative pattern.  

\begin{figure}[!htb]
\captionsetup{justification=centering}
\centering
\caption{Choice set manipulation adding a constant}
\includegraphics[width=0.9\textwidth]{./addconst.pdf}
\label{fig:addconstant_all}
\end{figure}

\begin{figure}[!htb]
\captionsetup{justification=centering}
\centering
\caption{Choice set manipulation multiplication by constant}
\includegraphics[width=0.9\textwidth]{./multiply.pdf}
\label{fig:multiply_all}
\end{figure}

%\FloatBarrier

\textbf{Multiplying by a constant.} Figure \ref{fig:multiply_all} shows the effect of multiplying each value by a constant. The global maximum rule predicts that this value manipulation results in the best option becoming relatively more attractive compared to the other two, whereas none of the other three value transformation rules predict any change in the choice proportions. The results from both experiments are in line with the prediction that the best option will increase its choice proportion share at the expense of the other two, strongly supporting the global maximum rule.



\textbf{Distant versus close second.} Figure \ref{fig:distclose2nd_all} shows the results from the close versus distant second choice set manipulation. Only the rank rule does not predict any change in the choice proportions, while the other three rules predict that the middle option becomes relatively more attractive. Results from both experiments support the latter prediction. Therefore, results from the distant versus close second choice manipulation provides strong evidence against the rank value transformation rule. 


\begin{figure}[!htb]
\captionsetup{justification=centering}
\centering
\caption{Choice set manipulation close versus distant middle}
\includegraphics[width=0.9\textwidth]{./closedistsec.pdf}
\label{fig:distclose2nd_all}
\end{figure}

\textbf{Distant versus close third.} Finally, Figure \ref{fig:closedist3rd_all} shows the results from the close versus distant third choice set manipulation. The predictions of the value transformation rules differ significantly, the two maximum rules predicting that the worst option will benefit at the expense of the other two (albeit the extent of this change slightly differs between the two rules), and the range rule predicting that both the worst and best options will steal from the middle item's share. The rank rule does not predict any change.  While the results from Experiment 2 unequivocally support the predictions of the two maximum rules, the results form Experiment 1 are more mixed (although we can detect a clear increase in the choice proportion of the  middle item). Therefore, results from the close versus distant third choice manipulation offers support for the two maximum rules.

\begin{figure}[!htb]
\captionsetup{justification=centering}
\centering
\caption{Choice set manipulation close versus distant third}
\includegraphics[width=0.9\textwidth]{./closedistthird.pdf}
\label{fig:closedist3rd_all}
\end{figure}


To summarize, the results from these qualitative comparisons suggest that the two maximum normalisation rules had the highest explanatory power. More specifically, albeit to varying degrees, but both maximum rules predicted the effect of three out of the four choice set manipulations, the range rule successfully predicted one, and the rank rule did not predict the effect of any of the choice set manipulations.

These results can be contrasted with the results from the model fitting approach, which indicated that the global maximum rule has the highest explanatory power by far. In line with this, multiplication by a constant, which was only correctly predicted by the global maximum rule, resulted in the largest difference between choice proportions in Experiment 1. However, it is also clear from the results of the qualitative comparison that the global maximum rule cannot alone explain choice behaviour.


In addition, interestingly, even though the two experiments involved fundamentally different stimuli (complex versus perceptual), the effects of the choice set manipulations on the choice proportions turned out to be rather similar. Unfortunately, a strict comparison of the two datasets is hindered by the relatively small sample size of Experiment 1. Nevertheless, the results lend some support to the idea that there exists a common valuation mechanism across a range of stimuli domains.



\newpage

\section{Discussion}

In this research project, our aim was to conduct a comprehensive evaluation of the relative explanatory power of four different forms of context-dependency. To compare four value transformation rules, each of which captures a different form of context-dependent valuation, we used a popular cognitive model, the DDM and its extension, the aDDM, to derive predictions about choice behaviour in two choice experiments with trinary choice sets, one with complex stimuli (movie posters), and another one with a perceptual task.

We tested the relative explanatory power of these value transformation rules in two ways. First, we used a model fitting approach, using choice and eye-tracking data from Experiment 1. Second, we compared the qualitative patterns of choice proportions using data from Experiment 2 and 1. The results from the two comparison methods were broadly in line, both supporting the view that while subjective valuation mostly reflects the absolute magnitude of the options under consideration, to a lesser extent, it is also affected by the relative value of the options (within the local context).

These findings are consistent with insights from research investigating the neural correlates of value-based decision making. Results from numerous neuroeconomic studies strongly support the view that the orbifrontal cortex (OFC) is the brain region where subjective value encoding takes place in economic choices (for a review see \citeNP{Padoa-Schioppa2017}). More importantly, it has been suggested that there is a group of neurons in the OFC with two fundamental properties that are likely to be directly responsible for simultaneous absolute and relative value sensitivity in economic valuation.

First, \citeA{Padoa-Schioppa2008} have shown that such neurons exhibit menu invariance, meaning that the value assigned to each option under consideration is independent from the value of the rest of the available alternatives, reflecting the absolute value of the option (global maximum in our experiments). Menu invariance gives rise to preference transitivity, ensuring that preferences are stable across the wide range of contexts the decision maker might encounter. Second, there is ample evidence that neuronal firing rates adapt to the range of available values (e.g., \citeNP{Padoa-Schioppa2009}; \citeNP{Louie2013}). Such adaptation is widespread in sensory systems, and is a natural consequence of biophysical constraints. Interestingly, although the most commonly proposed form of adaptation in the neuroeconomic literature is range adaptation (e.g., \citeNP{Soltani2012}), our data suggests that normalising by the maximum on the current trials fares better than a range normalisation approach at predicting choice.


There exist other ways to model context-sensitivity within the sequential sampling framework. Another approach to investigate context-dependency could have been to directly model changes in the drift rate. This could be done within an experiment where the range of values encountered are manipulated block by block (e.g., as done in \citeNP{Mullett2013}), and the model fitting approach can evaluate the explanatory power of various forms of context-dependency.

In addition, we could have investigated mixtures of models, for example, a hybrid model, where the subjective value is partly affected by the absolute value and partly affected by the local maximum transformation. Such investigations would have required estimation an additional, mixing parameter, which determines the degree to which each rule affects the subjective value.


 Alternatively, instead of changing the input values of the accumulation equation, we could have focused on how these values are integrated and incorporated into the accumulation process (described in Equations \ref{REA1}-\ref{REA3}). This is what \citeA{Teodorescu2016} did in a somewhat similar investigation to ours. Specifically, they contrasted relative and absolute evidence processing in a sequential sampling framework by comparing an independent race model (capturing absolute value processing, where the input is the absolute value of the options), with a DDM model (where differences of input values govern the accumulation process).

In their experiment, participants were instructed to choose the brighter out of two, fluctuating grey patches, with a fixed mean brightness. They focused on two manipulations: in an additive-boost condition, they added the same constant to both means, preserving the difference between the two mean brightness, whereas in the multiplicative-boost condition, they multiplied both means by the same constant, preserving the ratio of the two means. These conditions are direct equivalents to our add a constant, and multiply by a constant choice set manipulations.

Interestingly, they found that no  ``pure'' (either entirely absolute or relative) accumulation model could account for the data, and thus they propose two distinct types  of models that can account for this pattern: a DDM model where the noise in the process is a function of the intensity of the inputs, and a leaky competing accumulator model (LCA; \citeNP{Usher2001}), where simultaneous absolute and relative value sensitivity is a result of lateral inhibition. 

While our approach is different from that of \citeA{Teodorescu2016}, our results are similar, as they both suggest that subjective valuation are sensitive to both the absolute and relative magnitudes of objective values. This dovetails with findings from neuroeconomic research that suggests that OFC neurons exhibit both menu invariance and range adaptation. Taken together, these results point to a sequential sampling model with some form of hybrid value transformation rule. As an ever increasing amount of research focuses on understanding the neural basis of economic decision making, insights from this field will no doubt inform and greatly advance the explanatory power of cognitive models of choice in the future.

\bibliographystyle{apacite}

\newpage

\bibliography{refs}

\clearpage

\section*{Appendix}

\begin{table}[ht]
\centering
\caption{Best fitting parameter value for each participant and value transformation rule, simulations method} 
\scalebox{0.65}{
\begin{tabular}{@{\extracolsep{4pt}}cccccccccccccc}
\toprule   
{} & {} & \multicolumn{3}{c}{Global Max} & \multicolumn{3}{c}{Local Max}  & \multicolumn{3}{c}{Range} & \multicolumn{3}{c}{Rank} \\
 \cline{3-5} 
 \cline{6-8} 
 \cline{9-11} 
 \cline{12-14} 
 \\
 Participant  & Best fitting rule & $\theta$ & $\sigma$ & $d$ & $\theta$ & $\sigma$ & $d$ & $\theta$ & $\sigma$ & $d$ & $\theta$ & $\sigma$ & $d$\\ 
   \hline
  \\
 1 & Global Max & 0.75 & 0.19 & 0.32 & 0.76 & 0.20 & 0.26 & 0.86 & 0.20 & 0.10 & 0.64 & 0.21 & 0.12 \\ 
  2 & Rank & 0.57 & 0.24 & 0.71 & 0.68 & 0.23 & 0.32 & 0.61 & 0.22 & 0.18 & 0.58 & 0.23 & 0.20 \\ 
  3 & Local Max & 0.68 & 0.22 & 0.49 & 0.68 & 0.22 & 0.36 & 0.54 & 0.23 & 0.22 & 0.59 & 0.23 & 0.21 \\ 
  4 & Global Max & 0.66 & 0.26 & 0.30 & 0.66 & 0.26 & 0.21 & 0.59 & 0.26 & 0.11 & 0.65 & 0.26 & 0.10 \\ 
  5 & Global Max & 0.76 & 0.20 & 0.39 & 0.75 & 0.20 & 0.33 & 0.86 & 0.20 & 0.17 & 0.84 & 0.20 & 0.18 \\ 
  6 & Local Max & 0.63 & 0.24 & 0.32 & 0.48 & 0.23 & 0.31 & 0.65 & 0.24 & 0.21 & 0.60 & 0.24 & 0.22 \\ 
  7 & Rank & 0.63 & 0.17 & 0.22 & 0.65 & 0.17 & 0.20 & 0.47 & 0.16 & 0.15 & 0.48 & 0.16 & 0.15 \\ 
  8 & Range & 0.78 & 0.22 & 0.53 & 0.77 & 0.23 & 0.41 & 0.61 & 0.20 & 0.26 & 0.55 & 0.21 & 0.27 \\ 
  9 & Global Max & 0.86 & 0.18 & 0.21 & 0.88 & 0.19 & 0.17 & 0.54 & 0.18 & 0.15 & 0.76 & 0.19 & 0.12 \\ 
  10 & Global Max & 0.54 & 0.23 & 0.25 & 0.54 & 0.23 & 0.22 & 0.19 & 0.23 & 0.18 & 0.19 & 0.24 & 0.17 \\ 
  11 & Global Max & 0.67 & 0.13 & 0.26 & 0.64 & 0.14 & 0.18 & 0.64 & 0.15 & 0.10 & 0.59 & 0.15 & 0.11 \\ 
  12 & Global Max & 0.82 & 0.21 & 0.31 & 0.74 & 0.21 & 0.27 & 0.69 & 0.23 & 0.15 & 0.63 & 0.23 & 0.16 \\ 
  13 & Global Max & 0.45 & 0.19 & 0.31 & 0.40 & 0.19 & 0.26 & 0.17 & 0.20 & 0.18 & 0.21 & 0.20 & 0.17 \\ 
  14 & Global Max & 0.42 & 0.38 & 0.62 & 0.25 & 0.39 & 0.43 & 0.17 & 0.40 & 0.34 & 0.23 & 0.40 & 0.35 \\ 
  15 & Global Max & 0.63 & 0.14 & 0.18 & 0.53 & 0.15 & 0.16 & 0.52 & 0.14 & 0.11 & 0.48 & 0.15 & 0.11 \\ 
  16 & Global Max & 0.84 & 0.18 & 0.25 & 0.90 & 0.18 & 0.20 & 0.91 & 0.18 & 0.10 & 0.99 & 0.19 & 0.09 \\ 
  17 & Global Max & 0.79 & 0.26 & 0.71 & 0.79 & 0.27 & 0.57 & 0.59 & 0.27 & 0.29 & 0.48 & 0.26 & 0.31 \\ 
  18 & Global Max & 0.68 & 0.29 & 0.58 & 0.58 & 0.30 & 0.42 & 0.50 & 0.31 & 0.27 & 0.48 & 0.30 & 0.28 \\ 
  19 & Global Max & 0.62 & 0.19 & 0.32 & 0.58 & 0.18 & 0.26 & 0.57 & 0.18 & 0.19 & 0.51 & 0.18 & 0.20 \\ 
  20 & Global Max & 1.05 & 0.21 & 0.35 & 1.00 & 0.21 & 0.28 & 0.96 & 0.22 & 0.18 & 0.95 & 0.22 & 0.18 \\ 
  21 & Global Max & 0.59 & 0.22 & 0.32 & 0.53 & 0.22 & 0.31 & 0.28 & 0.21 & 0.28 & 0.34 & 0.21 & 0.26 \\ 
  22 & Range & 0.88 & 0.11 & 0.15 & 0.84 & 0.11 & 0.13 & 0.59 & 0.11 & 0.08 & 0.49 & 0.10 & 0.09 \\ 
  23 & Global Max & 0.70 & 0.14 & 0.15 & 0.75 & 0.14 & 0.12 & 0.53 & 0.14 & 0.09 & 0.49 & 0.14 & 0.10 \\ 
  24 & Local Max & 1.12 & 0.25 & 0.36 & 1.09 & 0.24 & 0.26 & 1.14 & 0.25 & 0.12 & 1.10 & 0.25 & 0.13 \\ 
  25 & Local Max & 0.22 & 0.21 & 0.12 & 0.20 & 0.21 & 0.09 & 0.22 & 0.21 & 0.07 & 0.24 & 0.21 & 0.08 \\ 
  26 & Global Max & 0.92 & 0.18 & 0.32 & 0.92 & 0.18 & 0.26 & 0.84 & 0.17 & 0.18 & 0.90 & 0.17 & 0.17 \\ 
  27 & Global Max & 0.48 & 0.12 & 0.18 & 0.48 & 0.13 & 0.15 & 0.23 & 0.14 & 0.10 & 0.20 & 0.14 & 0.10 \\ 
  28 & Local Max & 0.18 & 0.16 & 0.12 & 0.19 & 0.16 & 0.10 & 0.18 & 0.16 & 0.08 & 0.18 & 0.17 & 0.08 \\ 
  29 & Global Max & 0.27 & 0.19 & 0.28 & 0.25 & 0.20 & 0.18 & 0.19 & 0.20 & 0.16 & 0.20 & 0.20 & 0.16 \\ 
  30 & Global Max & 0.57 & 0.29 & 0.79 & 0.54 & 0.29 & 0.62 & 0.15 & 0.31 & 0.38 & 0.11 & 0.31 & 0.39 \\ 
  31 & Range & 0.25 & 0.22 & 0.01 & 0.30 & 0.22 & 0.00 & 0.26 & 0.21 & -0.03 & 0.27 & 0.22 & -0.03 \\ 
  32 & Range & 0.72 & 0.20 & 0.34 & 0.59 & 0.19 & 0.25 & 0.24 & 0.20 & 0.16 & 0.23 & 0.20 & 0.16 \\ 
  33 & Local Max & 0.78 & 0.21 & 0.25 & 0.68 & 0.21 & 0.22 & 0.57 & 0.21 & 0.12 & 0.57 & 0.20 & 0.14 \\ 
  34 & Local Max & 0.36 & 0.14 & 0.18 & 0.20 & 0.12 & 0.14 & 0.16 & 0.13 & 0.10 & 0.16 & 0.13 & 0.10 \\ 
  35 & Global Max & 0.36 & 0.32 & 0.41 & 0.49 & 0.32 & 0.30 & 0.24 & 0.33 & 0.24 & 0.28 & 0.32 & 0.26 \\ 
  36 & Range & 0.84 & 0.16 & 0.22 & 0.80 & 0.16 & 0.22 & 0.91 & 0.16 & 0.10 & 0.86 & 0.16 & 0.11 \\ 
  37 & Global Max & 0.73 & 0.16 & 0.31 & 0.79 & 0.16 & 0.22 & 0.80 & 0.17 & 0.11 & 0.89 & 0.17 & 0.10 \\ 
  38 & Local Max & 0.57 & 0.22 & 0.23 & 0.56 & 0.22 & 0.23 & 0.23 & 0.22 & 0.17 & 0.24 & 0.22 & 0.16 \\ 
  39 & Global Max & 0.15 & 0.38 & 0.23 & 0.23 & 0.38 & 0.11 & 0.28 & 0.37 & -0.01 & 0.33 & 0.37 & 0.02 \\ 
  40 & Global Max & 0.70 & 0.16 & 0.35 & 0.64 & 0.16 & 0.23 & 0.26 & 0.17 & 0.15 & 0.24 & 0.17 & 0.15 \\ 
  41 & Rank & 0.97 & 0.17 & 0.21 & 0.97 & 0.17 & 0.19 & 1.03 & 0.17 & 0.10 & 1.09 & 0.16 & 0.10 \\ 
  42 & Global Max & 0.81 & 0.19 & 0.33 & 0.85 & 0.19 & 0.25 & 1.04 & 0.20 & 0.12 & 1.02 & 0.20 & 0.11 \\ 
  43 & Local Max & 0.69 & 0.19 & 0.31 & 0.74 & 0.19 & 0.23 & 0.51 & 0.21 & 0.14 & 0.51 & 0.21 & 0.15 \\ 
  44 & Global Max & 0.60 & 0.22 & 0.32 & 0.52 & 0.22 & 0.28 & 0.42 & 0.24 & 0.19 & 0.44 & 0.23 & 0.18 \\ 
  45 & Rank & 0.97 & 0.20 & 0.49 & 0.96 & 0.20 & 0.42 & 0.76 & 0.19 & 0.27 & 0.81 & 0.18 & 0.27 \\ 
  46 & Global Max & 0.74 & 0.17 & 0.30 & 0.72 & 0.17 & 0.23 & 0.52 & 0.17 & 0.15 & 0.52 & 0.17 & 0.15 \\ 
  47 & Range & 1.02 & 0.24 & 0.30 & 0.99 & 0.26 & 0.25 & 1.06 & 0.24 & 0.15 & 1.02 & 0.24 & 0.16 \\ 
  48 & Global Max & 0.97 & 0.22 & 0.34 & 1.03 & 0.23 & 0.29 & 1.07 & 0.23 & 0.12 & 1.11 & 0.24 & 0.11 \\ 
  49 & Local Max & 0.40 & 0.20 & 0.48 & 0.46 & 0.19 & 0.31 & 0.21 & 0.21 & 0.22 & 0.24 & 0.21 & 0.24 \\ 
  50 & Local Max & 0.82 & 0.18 & 0.31 & 0.76 & 0.18 & 0.27 & 0.68 & 0.18 & 0.18 & 0.86 & 0.17 & 0.17 \\ 

 \hline

\end{tabular}
}
\end{table}





\begin{table}[ht]
\centering
\caption{Best fitting parameter value for each participant and value transformation rule, simulations method} 
\scalebox{0.65}{
\begin{tabular}{@{\extracolsep{4pt}}cccccccccccccc}
\toprule   
{} & {} & \multicolumn{3}{c}{Global Max} & \multicolumn{3}{c}{Local Max}  & \multicolumn{3}{c}{Range} & \multicolumn{3}{c}{Rank} \\
 \cline{3-5} 
 \cline{6-8} 
 \cline{9-11} 
 \cline{12-14} 
 \\
 Participant  & Best fitting rule & $\theta$ & $\sigma$ & $d$ & $\theta$ & $\sigma$ & $d$ & $\theta$ & $\sigma$ & $d$ & $\theta$ & $\sigma$ & $d$\\ 
   \hline
  \\
1 & Global Max & 0.78 & 0.19 & 0.31 & 0.78 & 0.19 & 0.26 & 0.57 & 0.20 & 0.12 & 0.53 & 0.20 & 0.12 \\ 
  2 & Rank & 0.64 & 0.24 & 0.63 & 0.76 & 0.22 & 0.29 & 0.60 & 0.22 & 0.18 & 0.61 & 0.22 & 0.19 \\ 
  3 & Local Max & 0.70 & 0.22 & 0.46 & 0.68 & 0.22 & 0.34 & 0.50 & 0.22 & 0.22 & 0.50 & 0.22 & 0.21 \\ 
  4 & Global Max & 0.75 & 0.25 & 0.26 & 0.71 & 0.25 & 0.19 & 0.59 & 0.26 & 0.10 & 0.53 & 0.26 & 0.10 \\ 
  5 & Global Max & 0.73 & 0.19 & 0.38 & 0.72 & 0.19 & 0.32 & 0.64 & 0.19 & 0.20 & 0.63 & 0.19 & 0.21 \\ 
  6 & Local Max & 0.67 & 0.23 & 0.30 & 0.59 & 0.23 & 0.27 & 0.54 & 0.24 & 0.23 & 0.49 & 0.23 & 0.23 \\ 
  7 & Rank & 0.67 & 0.17 & 0.21 & 0.65 & 0.17 & 0.19 & 0.53 & 0.15 & 0.15 & 0.54 & 0.15 & 0.14 \\ 
  8 & Range & 0.74 & 0.21 & 0.51 & 0.74 & 0.21 & 0.41 & 0.59 & 0.19 & 0.27 & 0.72 & 0.21 & 0.21 \\ 
  9 & Global Max & 0.83 & 0.18 & 0.21 & 0.79 & 0.18 & 0.18 & 0.70 & 0.18 & 0.13 & 0.68 & 0.19 & 0.13 \\ 
  10 & Global Max & 0.61 & 0.22 & 0.23 & 0.60 & 0.23 & 0.20 & 0.47 & 0.23 & 0.13 & 0.44 & 0.23 & 0.12 \\ 
  11 & Global Max & 0.59 & 0.13 & 0.28 & 0.64 & 0.13 & 0.18 & 0.44 & 0.14 & 0.13 & 0.41 & 0.15 & 0.13 \\ 
  12 & Global Max & 0.74 & 0.20 & 0.32 & 0.74 & 0.20 & 0.26 & 0.63 & 0.22 & 0.16 & 0.59 & 0.23 & 0.16 \\ 
  13 & Global Max & 0.44 & 0.19 & 0.28 & 0.42 & 0.19 & 0.24 & 0.08 & 0.19 & 0.19 & 0.07 & 0.20 & 0.19 \\ 
  14 & Global Max & 0.51 & 0.36 & 0.50 & 0.42 & 0.37 & 0.36 & 0.19 & 0.37 & 0.28 & 0.17 & 0.37 & 0.28 \\ 
  15 & Global Max & 0.56 & 0.13 & 0.18 & 0.54 & 0.14 & 0.16 & 0.42 & 0.13 & 0.12 & 0.41 & 0.13 & 0.12 \\ 
  16 & Global Max & 0.88 & 0.18 & 0.24 & 0.89 & 0.18 & 0.20 & 0.90 & 0.18 & 0.10 & 0.86 & 0.18 & 0.10 \\ 
  17 & Global Max & 0.77 & 0.23 & 0.68 & 0.76 & 0.24 & 0.53 & 1.14 & 0.25 & 0.18 & 0.57 & 0.24 & 0.28 \\ 
  18 & Global Max & 0.68 & 0.27 & 0.52 & 0.64 & 0.28 & 0.38 & 0.43 & 0.29 & 0.26 & 0.42 & 0.29 & 0.26 \\ 
  19 & Global Max & 0.68 & 0.18 & 0.30 & 0.66 & 0.18 & 0.24 & 0.55 & 0.18 & 0.19 & 0.52 & 0.18 & 0.19 \\ 
  20 & Global Max & 1.02 & 0.20 & 0.34 & 0.95 & 0.20 & 0.28 & 0.91 & 0.21 & 0.17 & 0.92 & 0.21 & 0.17 \\ 
  21 & Range & 0.62 & 0.22 & 0.32 & 0.56 & 0.21 & 0.28 & 0.43 & 0.20 & 0.23 & 0.44 & 0.21 & 0.23 \\ 
  22 & Rank & 0.82 & 0.11 & 0.15 & 0.83 & 0.10 & 0.14 & 0.71 & 0.10 & 0.07 & 0.69 & 0.10 & 0.07 \\ 
  23 & Global Max & 0.71 & 0.13 & 0.15 & 0.71 & 0.14 & 0.12 & 0.54 & 0.14 & 0.09 & 0.54 & 0.14 & 0.09 \\ 
  24 & Local Max & 1.01 & 0.24 & 0.38 & 1.04 & 0.23 & 0.26 & 1.02 & 0.24 & 0.13 & 1.01 & 0.24 & 0.13 \\ 
  25 & Rank & 0.27 & 0.21 & 0.10 & 0.24 & 0.21 & 0.09 & 0.00 & 0.20 & 0.09 & 0.00 & 0.21 & 0.09 \\ 
  26 & Global Max & 0.91 & 0.16 & 0.31 & 0.91 & 0.17 & 0.26 & 0.90 & 0.17 & 0.17 & 0.89 & 0.17 & 0.17 \\ 
  27 & Local Max & 0.45 & 0.12 & 0.18 & 0.45 & 0.12 & 0.15 & 0.11 & 0.13 & 0.11 & 0.13 & 0.13 & 0.11 \\ 
  28 & Global Max & 0.04 & 0.16 & 0.12 & 0.10 & 0.15 & 0.11 & 0.00 & 0.16 & 0.09 & 0.00 & 0.16 & 0.09 \\ 
  29 & Global Max & 0.38 & 0.19 & 0.26 & 0.35 & 0.19 & 0.19 & 0.16 & 0.20 & 0.15 & 0.17 & 0.20 & 0.15 \\ 
  30 & Global Max & 0.67 & 0.29 & 0.66 & 0.67 & 0.30 & 0.51 & 0.35 & 0.31 & 0.27 & 0.34 & 0.31 & 0.27 \\ 
  31 & Rank & 0.32 & 0.21 & 0.00 & 0.32 & 0.21 & 0.00 & 0.30 & 0.21 & 0.00 & 0.30 & 0.21 & 0.00 \\ 
  32 & Range & 0.71 & 0.19 & 0.30 & 0.68 & 0.20 & 0.25 & 0.49 & 0.19 & 0.14 & 0.49 & 0.20 & 0.13 \\ 
  33 & Rank & 0.70 & 0.20 & 0.24 & 0.68 & 0.20 & 0.22 & 0.43 & 0.20 & 0.14 & 0.43 & 0.20 & 0.14 \\ 
  34 & Local Max & 0.20 & 0.12 & 0.18 & 0.21 & 0.11 & 0.14 & 0.00 & 0.12 & 0.12 & 0.00 & 0.12 & 0.12 \\ 
  35 & Global Max & 0.55 & 0.31 & 0.36 & 0.54 & 0.31 & 0.26 & 0.36 & 0.31 & 0.20 & 0.34 & 0.31 & 0.20 \\ 
  36 & Range & 0.80 & 0.16 & 0.23 & 0.80 & 0.16 & 0.22 & 0.71 & 0.17 & 0.12 & 0.64 & 0.16 & 0.13 \\ 
  37 & Global Max & 0.80 & 0.16 & 0.29 & 0.81 & 0.16 & 0.20 & 0.72 & 0.16 & 0.12 & 0.71 & 0.17 & 0.12 \\ 
  38 & Local Max & 0.61 & 0.22 & 0.23 & 0.61 & 0.21 & 0.23 & 0.38 & 0.22 & 0.14 & 0.37 & 0.22 & 0.14 \\ 
  39 & Global Max & 0.01 & 0.35 & 0.20 & 0.00 & 0.36 & 0.15 & 0.00 & 0.35 & 0.03 & 0.00 & 0.36 & 0.07 \\ 
  40 & Global Max & 0.66 & 0.16 & 0.35 & 0.61 & 0.16 & 0.23 & 0.28 & 0.16 & 0.15 & 0.28 & 0.16 & 0.15 \\ 
  41 & Rank & 0.93 & 0.16 & 0.21 & 0.94 & 0.16 & 0.19 & 0.90 & 0.15 & 0.12 & 0.91 & 0.15 & 0.12 \\ 
  42 & Global Max & 0.76 & 0.17 & 0.33 & 0.77 & 0.18 & 0.27 & 0.72 & 0.20 & 0.14 & 0.73 & 0.20 & 0.13 \\ 
  43 & Local Max & 0.80 & 0.19 & 0.29 & 0.79 & 0.19 & 0.21 & 0.69 & 0.19 & 0.13 & 0.68 & 0.19 & 0.13 \\ 
  44 & Global Max & 0.63 & 0.21 & 0.29 & 0.62 & 0.22 & 0.25 & 0.80 & 0.23 & 0.13 & 0.45 & 0.23 & 0.18 \\ 
  45 & Rank & 0.95 & 0.18 & 0.47 & 0.92 & 0.18 & 0.41 & 0.81 & 0.17 & 0.25 & 0.79 & 0.17 & 0.25 \\ 
  46 & Global Max & 0.75 & 0.16 & 0.27 & 0.74 & 0.17 & 0.22 & 0.57 & 0.17 & 0.14 & 0.60 & 0.16 & 0.14 \\ 
  47 & Range & 0.99 & 0.24 & 0.29 & 0.96 & 0.23 & 0.25 & 1.00 & 0.23 & 0.15 & 1.23 & 0.24 & 0.13 \\ 
  48 & Global Max & 0.93 & 0.22 & 0.33 & 0.95 & 0.22 & 0.28 & 0.91 & 0.23 & 0.13 & 0.91 & 0.23 & 0.12 \\ 
  49 & Local Max & 0.44 & 0.19 & 0.44 & 0.49 & 0.19 & 0.29 & 0.92 & 0.22 & 0.11 & 0.28 & 0.20 & 0.22 \\ 
  50 & Local Max & 0.82 & 0.17 & 0.30 & 0.80 & 0.17 & 0.25 & 0.69 & 0.17 & 0.18 & 0.71 & 0.17 & 0.18 \\ 

 \hline

\end{tabular}
}
\end{table}




\end{document}